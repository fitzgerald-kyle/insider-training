{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a97e50",
   "metadata": {},
   "source": [
    "## Training with XGBoost\n",
    "Why choose XGBoost instead of a random forest: https://medium.com/geekculture/xgboost-versus-random-forest-898e42870f30 \\\n",
    "A great reference for tuning xgboost:\n",
    "https://towardsdatascience.com/from-zero-to-hero-in-xgboost-tuning-e48b59bfaf58\n",
    "\n",
    "This notebook contains the training of an XGBoost regressor model on insider data, which attempts to predict the maximum 90-day percentage gain of a ticker whose insider(s) made a trade. We use a custom objective function that penalizes overestimates more harshly than underestimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b09665",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport my_functions\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from operator import itemgetter\n",
    "\n",
    "from my_functions import *\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2801c03-df34-444b-9343-2cf75e10c38e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepareForModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-d9c0a832a73d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mDAYS_TO_LOOK_BACK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m6\u001b[0m  \u001b[1;31m# used for calculating volume volatility and recent-trade counts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_and_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepareForModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/training_and_cv_data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mstartDate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_and_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFilingDate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDAYS_TO_LOOK_BACK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prepareForModel' is not defined"
     ]
    }
   ],
   "source": [
    "DAYS_TO_LOOK_BACK = 6  # used for calculating volume volatility and recent-trade counts\n",
    "\n",
    "train_and_cv = prepareForModel(pd.read_csv('data/training_and_cv_data.csv'))\n",
    "\n",
    "startDate = min(train_and_cv.FilingDate) + dt.timedelta(days=DAYS_TO_LOOK_BACK)\n",
    "endDate = max(train_and_cv.FilingDate)\n",
    "splitDate = startDate + dt.timedelta(days=int(0.95*(endDate-startDate).days))\n",
    "\n",
    "train_XY, train_X, train_Y = returnXandY(\n",
    "    train_and_cv, dt.date.isoformat(startDate), dt.date.isoformat(splitDate)\n",
    ")\n",
    "\n",
    "cv_XY, cv_X, cv_Y = returnXandY(\n",
    "    train_and_cv,  dt.date.isoformat(splitDate+dt.timedelta(days=1)), dt.date.isoformat(endDate)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a56ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train shape: {train_X.shape}')\n",
    "print(f'cv shape: {cv_X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43688bcb-49e6-4b59-bc94-1eee5d16a93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need this later.\n",
    "def plotLearningCurves(results, losses):\n",
    "    '''\n",
    "    Plot the XGBoost training and cross-validation learning curves for each loss.\n",
    "    \n",
    "    IN:\n",
    "        results (obtained from an XGBoost fit)\n",
    "        losses (List[str]): contains name of each validation metric\n",
    "    '''\n",
    "    fig, axs = plt.subplots(1, len(losses), figsize=(6.4*len(losses), 4.8))\n",
    "    if type(axs) != np.ndarray: axs = [axs]\n",
    "        \n",
    "    for i, name in enumerate(losses):\n",
    "        axs[i].plot(results['validation_0'][name], label='train')\n",
    "        axs[i].plot(results['validation_1'][name], label='cv')\n",
    "        axs[i].set_xlabel('Epoch')\n",
    "        axs[i].set_ylabel('Loss')\n",
    "        axs[i].set_title('Learning Curve: ' + name)\n",
    "        axs[i].legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10654c38-58f8-4837-aa5f-ad42313687dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Perform standard feature normalization.\n",
    "'''\n",
    "#scaler = StandardScaler()\n",
    "#train_X_scaled = scaler.fit_transform(train_X)\n",
    "#cv_X_scaled = scaler.fit_transform(cv_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0ab75-b49a-41e3-9ae7-efc697c98e4e",
   "metadata": {},
   "source": [
    "Let's create an XGBRegressor object and tune the hyperparameters with RandomizedSearchCV to help performance on cv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb609bd0-d8b6-4bae-9122-634d2e35de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parameter grid\n",
    "parameters = {\n",
    "    'learning_rate': [1e-2, 5e-3, 1e-3],     # smaller = more conservative\n",
    "    'min_split_loss' : [0.1, 1., 2., 5.],  # larger = more conservative\n",
    "    'max_depth': [6, 8, 10, 12, 14, 16],           \n",
    "    'max_delta_step': [0., 1., 5., 10.],   # can help with imbalanced classes (e.g. more buys than sells)\n",
    "    'colsample_bytree': [0.6, 0.8, 1.],\n",
    "    'subsample': [0.5, 0.6, 0.7],           # lower values should help with overfitting\n",
    "    'reg_lambda': [1., 2., 5., 10.],          # regularization controls overfitting\n",
    "    'min_child_weight': [0., 1., 3., 5.],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327dd3b9-c199-4ed7-94df-ba082bc53658",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_rscv = RandomizedSearchCV(\n",
    "    xgb.XGBRegressor(\n",
    "        verbosity=0, early_stopping_rounds=100, n_estimators=1000, tree_method='hist'\n",
    "    ),\n",
    "    param_distributions=parameters,\n",
    "    verbose=0,\n",
    "    random_state=40\n",
    ")\n",
    "\n",
    "xgb_rscv_model = xgb_rscv.fit(\n",
    "    train_X, \n",
    "    train_Y, \n",
    "    eval_set=[(train_X, train_Y), (cv_X, cv_Y)],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf57f380-8463-44dc-ad3f-91be9e576567",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best learning_rate, min_split_loss, max_depth, max_delta_step, colsample_bytree, subsample,\\n' +\n",
    "      'reg_lambda, min_child_weight:\\n' +\n",
    "      f'''{itemgetter(\n",
    "        'learning_rate', 'min_split_loss', 'max_depth', 'max_delta_step', 'colsample_bytree', 'subsample', \n",
    "        'reg_lambda', 'min_child_weight'\n",
    "       )(xgb_rscv_model.best_estimator_.get_params())\n",
    "      }''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4df646-52b9-441a-8101-94861453b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model_best = xgb.XGBRegressor(**xgb_rscv_model.best_estimator_.get_params()).fit(\n",
    "    train_X, train_Y, eval_set=[(train_X, train_Y), (cv_X, cv_Y)], verbose=0\n",
    ")\n",
    "\n",
    "results = xgb_model_best.evals_result()\n",
    "\n",
    "plotLearningCurves(results, ['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f80f355-0381-46d7-ac67-ffa0679005eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(xgb_model_best)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d0f31-3ba3-4d9d-9af5-3d751d06e63a",
   "metadata": {},
   "source": [
    "### Let's visualize our predictions vs. the actual price percentage increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd0162b-ebec-439d-8cca-ac00088b7309",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y_preds = xgb_model_best.predict(train_X)\n",
    "plotPredictedVsActual(train_Y_preds, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2449d354-4460-4016-bb66-34de23192ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_Y_preds = xgb_model_best.predict(cv_X)\n",
    "plotPredictedVsActual(cv_Y_preds, cv_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edacb78-af7d-4e2e-aa65-a9492b255692",
   "metadata": {},
   "source": [
    "### Everything below the dotted line is an overestimate.\n",
    "\n",
    "That's not good: we don't want to be confident that a stock will gain a lot, only for it to underperform! On the contrary, if a stock outperforms our expectations and we've bought into it at all, then that's great!\n",
    "\n",
    "Let's define a custom objective function that penalizes overestimates more heavily than underestimates. It will be an asymmetric mean squared error where we penalize overestimates, say, 10 times more than underestimates. We want to be ***really*** confident that a ticker will gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9285ca-2782-4209-8387-b38f267e0197",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNDERESTIMATE_BIAS = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c96defa-2539-46c7-a4e5-aa5b28d9178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asymmetric_loss(wgt):\n",
    "    '''This is our custom objective loss function that favors either underestimates (wgt > 1)\n",
    "    or overestimates (0 < wgt < 1).\n",
    "    \n",
    "    minBool is a boolean that specifies whether or not the loss is minimized, because XGBoost wants to maximize \n",
    "    '''\n",
    "    def asymm_rmse(train, y_pred):\n",
    "        # I'm honestly confused about when a DMatrix is passed to this function vs. an ndarray,\n",
    "        # so this is a cheeky patch\n",
    "        if type(train) != np.ndarray: train = dtrain.get_label()\n",
    "        \n",
    "        diff = wgt*(y_pred-train)**2 * (train < y_pred).astype(float) + \\\n",
    "                (y_pred-train)**2 * (train >= y_pred).astype(float)\n",
    "        \n",
    "        loss = np.sqrt(np.mean(diff, axis=-1))\n",
    "\n",
    "        return loss\n",
    "    return asymm_rmse\n",
    "\n",
    "def asymmetric_grads(wgt):\n",
    "    '''\n",
    "    Custom objective that penalizes overestimates more heavily if wgt > 1, and underestimates if 0 < wgt < 1.\n",
    "    loss(train-predt) = {wgt/2*(train-predt)^2 if train < predt; \n",
    "                        1/2*(train-predt)^2 if train >= predt}\n",
    "    '''\n",
    "    def custom_grads(train, predt):\n",
    "        grad = -wgt*(train-predt)*(train<predt).astype(float) - (train-predt)*(train>=predt).astype(float)\n",
    "        hess = wgt*(train<predt).astype(float) + (train>=predt).astype(float)\n",
    "        \n",
    "        return grad, hess\n",
    "    return custom_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cad78e-b5dd-4fcf-9707-3dc0320f6c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor(\n",
    "    verbosity=1, \n",
    "    objective=asymmetric_grads(UNDERESTIMATE_BIAS),\n",
    "    eval_metric=asymmetric_loss(UNDERESTIMATE_BIAS),\n",
    "    early_stopping_rounds=100,\n",
    "    n_estimators=1000,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "xgb_rscv_custom = RandomizedSearchCV(\n",
    "    xgb_reg, \n",
    "    param_distributions=parameters,\n",
    "    verbose=1, \n",
    "    random_state=40,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "xgb_rscv_model_custom = xgb_rscv_custom.fit(\n",
    "    train_X, train_Y, eval_set=[(train_X, train_Y), (cv_X, cv_Y)], verbose=0\n",
    ")\n",
    "\n",
    "print('\\nBest learning_rate, min_split_loss, max_depth, max_delta_step, colsample_bytree, subsample,\\n'\n",
    "      + 'reg_lambda, min_child_weight:\\n'\n",
    "      + f'''{itemgetter(\n",
    "        'learning_rate', 'min_split_loss', 'max_depth', 'max_delta_step', 'colsample_bytree', 'subsample', \n",
    "        'reg_lambda', 'min_child_weight'\n",
    "       )(xgb_rscv_model_custom.best_estimator_.get_params())\n",
    "      }'''\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b228a2fb-d9cf-4dbb-a886-2175c88711c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Select the model with the best parameters and plot the training and CV learning curves.\n",
    "'''\n",
    "xgb_model_custom_best = xgb.XGBRegressor(**xgb_rscv_model_custom.best_estimator_.get_params()).fit(\n",
    "    train_X, train_Y, eval_set=[(train_X, train_Y), (cv_X, cv_Y)], verbose=0\n",
    ")\n",
    "\n",
    "results_custom = xgb_model_custom_best.evals_result()\n",
    "\n",
    "plotLearningCurves(results_custom, ['asymm_rmse', 'rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964a901c-f69a-445d-a7e3-b4e644cddbc3",
   "metadata": {},
   "source": [
    "This might be a sign that we have overfit the training set. Let's compare the predicted and actual increases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47968bf-a538-4793-9510-807809d92380",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y_preds_custom = xgb_model_custom_best.predict(train_X)\n",
    "plotPredictedVsActual(train_Y_preds_custom, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871b298-c243-4392-838a-0edba190fb2f",
   "metadata": {},
   "source": [
    "TRAINING: the regressor definitely learned very well how to avoid overestimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211ec1e-8c6f-4950-a6ba-354f7a9c068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_Y_preds_custom = xgb_model_custom_best.predict(cv_X)\n",
    "plotPredictedVsActual(cv_Y_preds_custom, cv_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735bb071-4d64-443c-a5c3-1d9a234462d3",
   "metadata": {},
   "source": [
    "CV: Well, we've certainly missed out on many large runs, including the one that was 300%+... ouch.\n",
    "\n",
    "However, what's important is that a higher proportion of points are near or above the \"predicted = actual\" line, which means that our algorithm has become more conservative and hence favorable. \n",
    "\n",
    "It might be inevitable that we overfit the training data a bit, especially with so little data and only 16 features, as the stock market is so volatile and influenced by so many confounding factors.\n",
    "\n",
    "This makes me think that I should perhaps only trust the algorithm when it's ***really*** confident."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5690ab2-b8e2-4269-ad9a-bac2c258aa65",
   "metadata": {},
   "source": [
    "### Let's visualize our categorizations with a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c55acd-ea3a-475d-a8fb-b3f8f5f40abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnPriceLabels(priceChange, labels, benchmark):\n",
    "    '''\n",
    "    Categorizes a percentage price change via comparison to a benchmark S&P500 gain.\n",
    "    '''\n",
    "        \n",
    "    if priceChange < 0:\n",
    "        return labels[0]\n",
    "    elif (priceChange >= 0) and (priceChange < benchmark):\n",
    "        return labels[1]\n",
    "    elif (priceChange >= benchmark) and (priceChange < 2*benchmark):\n",
    "        return labels[2]\n",
    "    elif (priceChange >= 2*benchmark) and (priceChange < 3*benchmark):\n",
    "        return labels[3]\n",
    "    else:\n",
    "        return labels[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba6008-6631-414c-819b-9cafe4b0e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "benchmark = 5  # sets group spacing in confusion matrix\n",
    "\n",
    "labels = ['<0%', \n",
    "          f'0-{benchmark}%',\n",
    "          f'{benchmark}-{2*benchmark}%',\n",
    "          f'{2*benchmark}-{3*benchmark}%',\n",
    "          f'>{3*benchmark}%']\n",
    "\n",
    "pred_labels = [returnPriceLabels(y_pred, labels, benchmark) for y_pred in cv_Y_preds_custom]\n",
    "true_labels = [returnPriceLabels(y_true, labels, benchmark) for y_true in cv_Y.to_numpy()]\n",
    "\n",
    "confMat = confusion_matrix(true_labels, pred_labels, labels=labels)\n",
    "plt.figure(figsize = (8,8))\n",
    "sn.heatmap(pd.DataFrame(confMat, labels, labels), annot=True, fmt='g', cbar=False, vmin=0, vmax=200)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "plt.title('Price Change Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c205de7c-f2d3-4608-8987-6daacec8fd54",
   "metadata": {},
   "source": [
    "For something as volatile as the stock market, this is pretty decent. We're happy with predictions that generally fall on or below the diagonal, which indicates that we tend to underestimate.\n",
    "\n",
    "In particular, look at the rightmost two columns. Our conservative algorithm performs pretty well when it makes bold estimates! Of 97 trades whose tickers were predicted to run by at least 8%, 73 actually did.\n",
    "\n",
    "#### That's an 75% success rate on high-prediction trades!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b6b1dd-288c-4346-98e0-016aaeced30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphviz isn't on the environment's path by default for some reason, so we must add it\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'\n",
    "\n",
    "xgb.plot_tree(xgb_model_custom_best)  # plot the first tree in the model\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(150, 100)\n",
    "plt.show()\n",
    "fig.savefig('outputs/regression_tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82981538-be37-49bf-aa16-a3a89df32410",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(xgb_model_custom_best)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514feaaf-a6d4-4e8d-95ec-5f60827e672a",
   "metadata": {},
   "source": [
    "The feature importance map is interesting and a cause for some consideration -- current price is by far the most important indicator of future price.\n",
    "\n",
    "I think that this can be seen as either a bit concerning, or a bit tautological. On the one hand, we'd naturally not expect price to give any sort of indication of what the future price will be -- it's just an arbitrary value! But on the other hand, even though share structure is what's actually important in determining how easily the price can jump, I suspect that it usually is such that higher-priced tickers don't tend to run as easily as lower-priced tickers. Thus, it's just inherently more likely that a \\\\$1 ticker increases to \\\\$1.10 than a \\\\$100 ticker increases to \\\\$110.\n",
    "\n",
    "Quantity of shares owned by the insider is important, which makes sense. If they own more shares, they probably know more about the company, and their purchases and sells directly affect the ticker price more.\n",
    "\n",
    "Surprisingly, particular insider titles don't seem to matter that much. Something to consider is looking for particular kinds of clusters of trades, e.g. multiple recent trades with at least one of them being an officer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078536f0-5f86-4457-abd1-6484aca25e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_XY['XGB_Prediction'] = cv_Y_preds_custom\n",
    "save_obj(cv_XY, 'data/cv_XY')\n",
    "\n",
    "xgb_model_best.save_model('models/xgb_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed3951-e969-44b2-8509-9b634261b209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
