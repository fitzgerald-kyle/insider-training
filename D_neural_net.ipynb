{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55050497-5c36-4f51-a496-72be969269a0",
   "metadata": {},
   "source": [
    "## Training with a Neural Network\n",
    "This notebook contains the training of a Keras Sequential model on insider data, which attempts to predict the maximum 90-day percentage gain of a ticker whose insider(s) made a trade. We use a single dense hidden layer and a dropout layer, and we use the Keras Tuner to choose an optimal learning rate and number of hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b09665",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport my_functions\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from operator import itemgetter\n",
    "\n",
    "from my_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525a07b1-e849-4aaf-a3b8-7afa0cb2b071",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAYS_TO_LOOK_BACK = 6  # used for calculating volume volatility and recent-trade counts\n",
    "\n",
    "train_and_cv = prepareForModel(pd.read_csv('data/training_and_cv_data.csv'))\n",
    "\n",
    "startDate = min(train_and_cv.FilingDate) + dt.timedelta(days=DAYS_TO_LOOK_BACK)\n",
    "endDate = max(train_and_cv.FilingDate)\n",
    "splitDate = startDate + dt.timedelta(days=int(0.9*(endDate-startDate).days))\n",
    "\n",
    "train_XY, train_X, train_Y = returnXandY(train_and_cv, \n",
    "                                         dt.date.isoformat(startDate), \n",
    "                                         dt.date.isoformat(splitDate))\n",
    "\n",
    "cv_XY, cv_X, cv_Y = returnXandY(train_and_cv, \n",
    "                                dt.date.isoformat(splitDate+dt.timedelta(days=1)), dt.date.isoformat(\n",
    "                                    endDate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f335100-3104-4d2a-91f2-dc4f270210e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (4670, 16)\n",
      "cv shape: (521, 16)\n"
     ]
    }
   ],
   "source": [
    "print(f'train shape: {train_X.shape}')\n",
    "print(f'cv shape: {cv_X.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b828936-c6db-4a88-a517-f80936deae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Perform standard feature normalization.\n",
    "'''\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "cv_X_scaled = scaler.fit_transform(cv_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb894f1-e2e4-4c53-bb32-48fe30f1e5f7",
   "metadata": {},
   "source": [
    "Here is a StackExchange answer that provides a starting point for deciding on the number of hidden units to include: https://stats.stackexchange.com/a/136542\n",
    "\n",
    "### Below is a computation to obtain the absolute maximum number of hidden units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13153c21-12df-4657-ace4-ca672f1f67e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An upper bound for number of hidden units: 137\n"
     ]
    }
   ],
   "source": [
    "Ns = train_X_scaled.shape[0]  # training examples\n",
    "No = 1                           # output neurons\n",
    "Ni = train_X_scaled.shape[1]  # input neurons\n",
    "alpha = 2                        # scale factor\n",
    "\n",
    "Nh = Ns / (alpha*(Ni + No))      # maximum hidden neurons\n",
    "\n",
    "print(f'An upper bound for number of hidden units: {int(Nh)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c2a62b-e0b8-4899-bcce-7d0465084faa",
   "metadata": {},
   "source": [
    "This seems like *quite* a lot; we certainly don't need this many hidden units! \n",
    "\n",
    "### However, we can remove some of the guesswork by using Keras Tuner. \n",
    "With this tool, we can search the parameter space and also determine an optimal number of hidden units.\n",
    "\n",
    "We have 16 inputs, so let's opt for one Dense and one Dropout hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31ebcc42-8066-4646-9330-0fac84929292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import math_ops, numpy_ops\n",
    "numpy_ops.np_config.enable_numpy_behavior()\n",
    "\n",
    "def asymmetric_loss(wgt):\n",
    "    '''This is our custom objective loss function that favors either underestimates (wgt > 1)\n",
    "    or overestimates (0 < wgt < 1).'''\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        diff = wgt*math_ops.squared_difference(y_pred, y_true)*(y_true < y_pred).astype(float) + \\\n",
    "                math_ops.squared_difference(y_pred, y_true)*(y_true >= y_pred).astype(float)\n",
    "        \n",
    "        loss = tf.math.sqrt(tf.reduce_mean(diff, axis=-1))\n",
    "\n",
    "        return loss\n",
    "    return custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71a21264-d7c1-4c04-85ae-0226c2e9bf22",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "tf.random.set_seed(40)\n",
    "\n",
    "def model_builder(numFeatures):\n",
    "    def builder(tuner):\n",
    "        numUnits = tuner.Int('units', min_value=4, max_value=32, step=4)\n",
    "        learningRate = tuner.Choice('learningRate', values=[1e-2, 1e-3, 1e-4])\n",
    "        UNDERESTIMATE_BIAS = tuner.Choice('UNDERESTIMATE_BIAS', values=[1, 2, 5, 10, 20, 50])\n",
    "        \n",
    "        model = Sequential(\n",
    "            [               \n",
    "                Input(shape=(numFeatures,)),\n",
    "                Dense(units=numUnits, activation='relu', name='dense_1'),\n",
    "                Dropout(0.1),\n",
    "                Dense(units=1, activation='linear', name='dense_2')\n",
    "            ], name = 'nn_model' \n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=learningRate),\n",
    "            loss=asymmetric_loss(UNDERESTIMATE_BIAS),\n",
    "            metrics=asymmetric_loss(UNDERESTIMATE_BIAS)\n",
    "        )\n",
    "\n",
    "        return model\n",
    "    return builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82daf77f-9ee1-462d-a611-7e5602cc5b16",
   "metadata": {},
   "source": [
    "Hyperband is an algorithm that searches the hyperparameter space with respect to which we want to minimize the evaluation metric, i.e. the custom asymmetric loss. \n",
    "\n",
    "We use EarlyStopping to halt training early if there is no loss improvement in the 10 most recent epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ef163f2-77db-44c0-9e33-f67a3813e696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "4                 |?                 |units\n",
      "0.0001            |?                 |learningRate\n",
      "50                |?                 |UNDERESTIMATE_BIAS\n",
      "2                 |?                 |tuner/epochs\n",
      "0                 |?                 |tuner/initial_epoch\n",
      "3                 |?                 |tuner/bracket\n",
      "0                 |?                 |tuner/round\n",
      "\n",
      "Epoch 1/2\n",
      "146/146 [==============================] - 1s 2ms/step - loss: 428.3163 - custom_loss: 428.3163 - val_loss: 425.7496 - val_custom_loss: 425.7496\n",
      "Epoch 2/2\n",
      "146/146 [==============================] - 0s 1ms/step - loss: 427.6000 - custom_loss: 427.6000 - val_loss: 424.8866 - val_custom_loss: 424.8866\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected the return value of HyperModel.fit() to be a single float when `objective` is left unspecified. Recevied return value: <keras.callbacks.History object at 0x0000025BB23955E0> of type <class 'keras.callbacks.History'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-14a6241b6d04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mstopEarly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m tuner.search(\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mtrain_X_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m             \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m             \u001b[1;31m# `results` is None indicates user updated oracle in `run_trial()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_tuner\\tuners\\hyperband.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"epochs\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tuner/epochs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[0mfit_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"initial_epoch\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tuner/initial_epoch\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHyperband\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_build_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"callbacks\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m             \u001b[0mobj_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         tuner_utils.validate_trial_results(\n\u001b[0m\u001b[0;32m    224\u001b[0m             \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"HyperModel.fit()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner_utils.py\u001b[0m in \u001b[0;36mvalidate_trial_results\u001b[1;34m(results, objective, func_name)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m     ):\n\u001b[1;32m--> 300\u001b[1;33m         raise TypeError(\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;34mf\"Expected the return value of {func_name} to be \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[1;34m\"a single float when `objective` is left unspecified. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected the return value of HyperModel.fit() to be a single float when `objective` is left unspecified. Recevied return value: <keras.callbacks.History object at 0x0000025BB23955E0> of type <class 'keras.callbacks.History'>."
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    model_builder(train_X_scaled.shape[1]),\n",
    "    objective=asymmetric_loss(UNDERESTIMATE_BIAS),\n",
    "    max_epochs=30,\n",
    "    overwrite=True,\n",
    "    directory='tuner logs',\n",
    "    project_name='asymmetric_MSE'\n",
    ")\n",
    "\n",
    "stopEarly = tf.keras.callbacks.EarlyStopping(patience=10)\n",
    "\n",
    "tuner.search(\n",
    "    train_X_scaled, train_Y, \n",
    "    epochs=30,\n",
    "    validation_data=(cv_X_scaled, cv_Y),\n",
    "    callbacks=[stopEarly]\n",
    ")\n",
    "\n",
    "best_hparams = tuner.get_best_hyperparameters()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9b8459e7-cb83-4c07-8dea-da9fddf38565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the best hyperparameter values: \n",
      " {'units': 32, 'learningRate': 0.01, 'UNDERESTIMATE_BIAS': 1, 'tuner/epochs': 10, 'tuner/initial_epoch': 0, 'tuner/bracket': 1, 'tuner/round': 0}\n"
     ]
    }
   ],
   "source": [
    "print(f'These are the best hyperparameter values: \\n {best_hparams.values}')\n",
    "nn_model = tuner.hypermodel.build(best_hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb08d509-4793-406b-8523-dea17ada4412",
   "metadata": {},
   "source": [
    "We aren't surprised to see that loss is minimized for UNDERESTIMATE_BIAS=1, i.e. a symmetric mean squared error, because increasing the bias directly increases the loss accumulated from each overestimate. I initially thought about only allowing values greater than 1, but as will be seen below, we have another, more pressing problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "72fb598e-5c57-4a60-9c73-7500f4861c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148/148 [==============================] - 0s 822us/step\n",
      "17/17 [==============================] - 0s 742us/step\n",
      "\n",
      "cv MSE: 784.2179197510907\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_Y_preds = nn_model.predict(train_X_scaled)\n",
    "cv_Y_preds = nn_model.predict(cv_X_scaled)\n",
    "\n",
    "print(f'\\ncv MSE: {mean_squared_error(cv_Y, cv_Y_preds)}\\n')\n",
    "\n",
    "dfcv = pd.DataFrame(data={'cvPreds': np.reshape(cv_Y_preds, (cv_Y_preds.size,)), 'cvVals': cv_Y})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc30cf-23ea-4a42-b604-670740d3aea6",
   "metadata": {},
   "source": [
    "Note that this cv MSE is ~100 points higher than that produced by the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ac69f6d2-1fe3-41c5-9dc0-91bedd506c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 largest predictions and their values: \n",
      "        cvPreds      cvVals\n",
      "5153  10.747974   10.305895\n",
      "5156  10.728843   10.305895\n",
      "5157  10.154403   10.305895\n",
      "5245   1.661836  225.657188\n",
      "5246   1.661831  225.657188\n",
      "\n",
      "The 5 smallest predictions and their values: \n",
      "       cvPreds     cvVals\n",
      "5320 -2.920911   6.650825\n",
      "5101 -2.514010  63.048783\n",
      "5283 -2.264927  -2.651003\n",
      "5177 -2.098195  15.254239\n",
      "5435 -2.057703  28.778254\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "nlargest = dfcv.nlargest(n, ['cvPreds'])\n",
    "nsmallest = dfcv.nsmallest(n, ['cvPreds'])\n",
    "print(f'The {n} largest predictions and their values: \\n{nlargest}\\n')\n",
    "print(f'The {n} smallest predictions and their values: \\n{nsmallest}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8551abf5-cfe2-4d87-b879-a7398d3ef8ff",
   "metadata": {},
   "source": [
    "### Here, we've reached a bit of a standstill in the modeling process.\n",
    "Recall from above that these predictions are generated with UNDERESTIMATE_BIAS=1 -- there is no bias towards underestimates in our objective function! Yet all but 3 cross-validation estimates are well under 2%, and all but 3 estimates are in the interval \\[-3, 2\\]%.\n",
    "\n",
    "This is puzzling, and I need to think more about why this would be. These predictions are kind of useless -- using them to create an investment strategy probably wouldn't result in us losing money, but we also likely wouldn't gain much, especially relative to the S&P500 gaining 8% in this time period!\n",
    "\n",
    "However, the XGBoost model seems to be working better, so let's implement a strategy with that model in strategy_simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d31d5c3-fee1-45cf-85a8-08de4fc61396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
